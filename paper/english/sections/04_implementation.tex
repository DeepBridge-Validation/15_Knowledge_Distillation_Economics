\section{Implementation}

\subsection{Architecture}

\subsubsection{Technology Stack}

Implementation is based on:

\begin{itemize}
    \item \textbf{Python 3.9+}: Main language
    \item \textbf{DeepBridge}: Base distillation framework
    \item \textbf{statsmodels}: GAM implementation (GLMGam)
    \item \textbf{scikit-learn}: Linear models and infrastructure
    \item \textbf{NumPy/SciPy}: Numerical operations and statistical tests
    \item \textbf{Optuna}: Hyperparameter optimization
\end{itemize}

\subsubsection{Main Modules}

\begin{table}[h]
\centering
\caption{Econometric Framework Modules}
\small
\begin{tabularx}{\columnwidth}{lX}
\toprule
\textbf{Module} & \textbf{Functionality} \\
\midrule
\texttt{economics/constraints.py} & Constraint encoding and validation \\
\texttt{economics/distillation.py} & Distillation engine with constraints \\
\texttt{economics/stability.py} & Bootstrap stability analysis \\
\texttt{economics/breaks.py} & Structural break detection \\
\texttt{economics/metrics.py} & Specialized economic metrics \\
\texttt{economics/reporting.py} & Reports for economists \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Implementation of Economic Constraints}

\subsubsection{EconomicConstraints Class}

The \texttt{EconomicConstraints} class manages three types of constraints: (1) sign constraints (stored in \texttt{sign\_constraints} dict), (2) monotonicity constraints, and (3) magnitude bounds. The \texttt{evaluate\_violations()} method computes marginal effects and checks each constraint, returning a dictionary of violations with their types and magnitudes.

\subsubsection{Marginal Effects Calculation}

For GAM models, marginal effects are computed via numerical differentiation: $\frac{\partial f}{\partial x_i} \approx \frac{f(x + \epsilon) - f(x)}{\epsilon}$. For linear models, marginal effects equal coefficients directly.

\subsection{Distillation Engine with Constraints}

\subsubsection{EconomicDistiller Class}

The \texttt{EconomicDistiller} extends DeepBridge's \texttt{KnowledgeDistillation} class, adding constraint penalization. The \texttt{\_combined\_loss()} method computes three components: (1) KL divergence from teacher, (2) cross-entropy with true labels, and (3) constraint violation penalties. Training uses mini-batch gradient descent with sklearn's \texttt{warm\_start} for iterative optimization.

\subsection{Stability Analyzer}

\subsubsection{Bootstrap Implementation}

The \texttt{StabilityAnalyzer} performs $B=1000$ bootstrap resamples, fitting the student model on each. For each bootstrap iteration, coefficients are extracted (directly for linear models, via spline evaluation for GAMs). Stability metrics include: mean, standard deviation, coefficient of variation (CV), 95\% confidence intervals, and sign consistency across samples.

\subsection{Structural Break Detector}

\subsubsection{Rolling Window Analysis}

The \texttt{StructuralBreakDetector} uses rolling windows of size 500 with step 100. For each window, a student model is fitted and coefficients extracted. The \texttt{\_test\_breaks()} method computes Wald statistics $W = (\beta^{(t+1)} - \beta^{(t)})^T (\beta^{(t+1)} - \beta^{(t)})$ between consecutive windows, identifying breaks at $p < 0.05$ significance level.

\subsection{Economic Metrics}

\subsubsection{Specialized Economic Metrics}

The \texttt{EconomicMetrics} class provides three key metrics: (1) \textbf{constraint compliance rate} = $1 - |\text{violations}|/|\text{constraints}|$, (2) \textbf{marginal effect preservation} = average Pearson correlation between teacher and student marginal effects, and (3) \textbf{economic interpretability score} = weighted combination (0.4 compliance + 0.3 stability + 0.3 sign consistency), normalized to 0-100\%.

\subsection{Performance Optimizations}

\subsubsection{Caching and Parallelization}

Teacher probabilities are pre-computed and cached to avoid repeated inference during bootstrap. Bootstrap iterations are parallelized using \texttt{joblib.Parallel} with \texttt{n\_jobs=-1}, enabling multi-core execution that reduces 1000-iteration runtime from $\sim$130 hours to $\sim$8 hours on 16 cores.

\subsection{Integration with DeepBridge Workflow}

The framework integrates seamlessly with DeepBridge's existing pipeline through the \texttt{AutoDistiller} interface. Users load datasets via \texttt{DBDataset}, train teacher models with advanced methods (e.g., HPM), configure economic constraints, and generate comprehensive reports including coefficient stability analysis and structural break detection. The complete workflow requires $<$20 lines of code.
