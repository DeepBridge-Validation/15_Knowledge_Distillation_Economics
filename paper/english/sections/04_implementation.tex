\section{Implementation}

\subsection{Architecture}

\subsubsection{Technology Stack}

Implementation is based on:

\begin{itemize}
    \item \textbf{Python 3.9+}: Main language
    \item \textbf{DeepBridge}: Base distillation framework
    \item \textbf{statsmodels}: GAM implementation (GLMGam)
    \item \textbf{scikit-learn}: Linear models and infrastructure
    \item \textbf{NumPy/SciPy}: Numerical operations and statistical tests
    \item \textbf{Optuna}: Hyperparameter optimization
\end{itemize}

\subsubsection{Main Modules}

\begin{table}[h]
\centering
\caption{Econometric Framework Modules}
\small
\begin{tabularx}{\columnwidth}{lX}
\toprule
\textbf{Module} & \textbf{Functionality} \\
\midrule
\texttt{economics/constraints.py} & Constraint encoding and validation \\
\texttt{economics/distillation.py} & Distillation engine with constraints \\
\texttt{economics/stability.py} & Bootstrap stability analysis \\
\texttt{economics/breaks.py} & Structural break detection \\
\texttt{economics/metrics.py} & Specialized economic metrics \\
\texttt{economics/reporting.py} & Reports for economists \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Implementation of Economic Constraints}

\subsubsection{EconomicConstraints Class}

\begin{lstlisting}[language=Python, caption=Constraint Implementation]
class EconomicConstraints:
    def __init__(self):
        self.sign_constraints = {}
        self.monotonicity_constraints = {}
        self.magnitude_bounds = {}

    def add_sign(self, feature: str, sign: int,
                 justification: str = ""):
        """
        Args:
            feature: Variable name
            sign: +1 (positive) or -1 (negative)
            justification: Economic foundation
        """
        self.sign_constraints[feature] = {
            'sign': sign,
            'justification': justification
        }

    def evaluate_violations(self, model, X):
        """Calculate constraint violations"""
        violations = {}

        # Sign violations
        for feat, constraint in self.sign_constraints.items():
            marginal_effect = self._compute_marginal(
                model, X, feat
            )
            expected_sign = constraint['sign']
            actual_sign = np.sign(marginal_effect)

            if actual_sign != expected_sign:
                violations[feat] = {
                    'type': 'sign',
                    'expected': expected_sign,
                    'actual': actual_sign,
                    'magnitude': abs(marginal_effect)
                }

        # Monotonicity violations
        for feat, constraint in self.monotonicity_constraints.items():
            mono_violations = self._check_monotonicity(
                model, X, feat, constraint['direction']
            )
            if mono_violations > 0:
                violations[feat] = {
                    'type': 'monotonicity',
                    'count': mono_violations
                }

        return violations
\end{lstlisting}

\subsubsection{Marginal Effects Calculation}

For GAM models:
\begin{lstlisting}[language=Python]
def compute_marginal_effect_gam(model, X, feature, epsilon=1e-5):
    """Numerical approximation of marginal effect"""
    X_plus = X.copy()
    X_plus[feature] += epsilon

    pred_base = model.predict(X)
    pred_plus = model.predict(X_plus)

    marginal = (pred_plus - pred_base) / epsilon
    return np.mean(marginal)
\end{lstlisting}

For linear models:
\begin{lstlisting}[language=Python]
def compute_marginal_effect_linear(model, feature_index):
    """Marginal effect = coefficient"""
    return model.coef_[feature_index]
\end{lstlisting}

\subsection{Distillation Engine with Constraints}

\subsubsection{EconomicDistiller Class}

Extension of DeepBridge's \texttt{KnowledgeDistillation}:

\begin{lstlisting}[language=Python, caption=Econometric Distillation]
class EconomicDistiller(KnowledgeDistillation):
    def __init__(self, constraints: EconomicConstraints,
                 temperature: float = 2.0,
                 alpha: float = 0.5,
                 beta: float = 0.3):
        super().__init__(temperature=temperature, alpha=alpha)
        self.constraints = constraints
        self.beta = beta  # Constraint weight

    def _combined_loss(self, y_true, p_teacher, p_student, model, X):
        """Modified loss with constraint penalization"""
        # Standard distillation loss
        L_kd = self._kl_divergence(p_teacher, p_student)
        L_hard = self._cross_entropy(y_true, p_student)

        # Constraint penalization
        violations = self.constraints.evaluate_violations(model, X)
        L_constraint = sum(v['magnitude'] for v in violations.values())

        # Combined loss
        loss = (self.alpha * L_kd +
                (1 - self.alpha) * L_hard +
                self.beta * L_constraint)

        return loss, violations

    def fit(self, X, y, teacher_probs=None):
        """Train student model with constraints"""
        if teacher_probs is None:
            teacher_probs = self.teacher.predict_proba(X)

        # Initialize student (GAM or Linear)
        self._initialize_student()

        # Iterative optimization
        for epoch in range(self.n_epochs):
            for X_batch, y_batch, p_batch in self._get_batches(
                X, y, teacher_probs
            ):
                p_student = self.student.predict_proba(X_batch)

                loss, violations = self._combined_loss(
                    y_batch, p_batch, p_student,
                    self.student, X_batch
                )

                # Gradient descent (via sklearn warm_start)
                self.student.partial_fit(X_batch, y_batch)

                # Log violations
                self._log_violations(epoch, violations)

        return self.student
\end{lstlisting}

\subsection{Stability Analyzer}

\subsubsection{Bootstrap Implementation}

\begin{lstlisting}[language=Python, caption=Stability Analysis]
class StabilityAnalyzer:
    def __init__(self, n_bootstrap: int = 1000,
                 confidence_level: float = 0.95):
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level

    def analyze(self, distiller, X, y, teacher_probs):
        """Analyze stability via bootstrap"""
        n_samples = len(X)
        coefficients = []

        for b in tqdm(range(self.n_bootstrap)):
            # Bootstrap sample
            indices = np.random.choice(
                n_samples, size=n_samples, replace=True
            )
            X_boot = X[indices]
            y_boot = y[indices]
            p_boot = teacher_probs[indices]

            # Fit student on bootstrap sample
            student = distiller.fit(X_boot, y_boot, p_boot)

            # Extract coefficients
            if hasattr(student, 'coef_'):
                coef = student.coef_
            else:
                # For GAM: extract spline coefficients
                coef = self._extract_gam_effects(student, X)

            coefficients.append(coef)

        # Compute stability metrics
        coefficients = np.array(coefficients)
        results = {
            'mean': np.mean(coefficients, axis=0),
            'std': np.std(coefficients, axis=0),
            'cv': self._compute_cv(coefficients),
            'ci_lower': np.percentile(coefficients, 2.5, axis=0),
            'ci_upper': np.percentile(coefficients, 97.5, axis=0),
            'sign_stability': self._compute_sign_stability(coefficients)
        }

        return results

    def _compute_cv(self, coefficients):
        """Coefficient of variation"""
        mean = np.mean(np.abs(coefficients), axis=0)
        std = np.std(coefficients, axis=0)
        return std / (mean + 1e-10)

    def _compute_sign_stability(self, coefficients):
        """Proportion of samples with consistent sign"""
        signs = np.sign(coefficients)
        mode_sign = stats.mode(signs, axis=0)[0]
        stability = np.mean(signs == mode_sign, axis=0)
        return stability
\end{lstlisting}

\subsection{Structural Break Detector}

\subsubsection{Rolling Window Analysis}

\begin{lstlisting}[language=Python, caption=Break Detection]
class StructuralBreakDetector:
    def __init__(self, window_size: int = 500,
                 step_size: int = 100):
        self.window_size = window_size
        self.step_size = step_size

    def detect(self, X, y, teacher_probs, time_var):
        """Detect structural breaks in time series"""
        # Sort by time
        sorted_idx = np.argsort(X[time_var])
        X_sorted = X.iloc[sorted_idx]
        y_sorted = y[sorted_idx]
        p_sorted = teacher_probs[sorted_idx]

        # Rolling windows
        windows = []
        coefficients = []

        for start in range(0, len(X) - self.window_size,
                          self.step_size):
            end = start + self.window_size

            X_window = X_sorted.iloc[start:end]
            y_window = y_sorted[start:end]
            p_window = p_sorted[start:end]

            # Fit student in window
            distiller = EconomicDistiller(...)
            student = distiller.fit(X_window, y_window, p_window)

            # Extract coefficients
            coef = self._extract_coefficients(student)

            windows.append((start, end))
            coefficients.append(coef)

        # Test for structural breaks
        breaks = self._test_breaks(coefficients)

        return {
            'windows': windows,
            'coefficients': coefficients,
            'breaks': breaks
        }

    def _test_breaks(self, coefficients):
        """Wald test for structural breaks"""
        coefficients = np.array(coefficients)
        breaks = []

        for t in range(len(coefficients) - 1):
            coef_t = coefficients[t]
            coef_t1 = coefficients[t + 1]

            # Wald statistic
            diff = coef_t1 - coef_t
            # Simplified: use identity as cov matrix
            W = np.sum(diff ** 2)

            # Chi-squared test
            p_value = 1 - stats.chi2.cdf(W, df=len(diff))

            if p_value < 0.05:
                breaks.append({
                    'window': t,
                    'statistic': W,
                    'p_value': p_value,
                    'changed_features': self._identify_changed_features(diff)
                })

        return breaks
\end{lstlisting}

\subsection{Economic Metrics}

\subsubsection{Specialized Economic Metrics}

\begin{lstlisting}[language=Python, caption=Specialized Metrics]
class EconomicMetrics:
    @staticmethod
    def constraint_compliance_rate(model, constraints, X):
        """Rate of conformity with economic constraints"""
        violations = constraints.evaluate_violations(model, X)
        total_constraints = len(constraints.sign_constraints) + \
                          len(constraints.monotonicity_constraints)
        compliance_rate = 1 - (len(violations) / total_constraints)
        return compliance_rate

    @staticmethod
    def marginal_effect_preservation(teacher, student, X, features):
        """Preservation of marginal effects vs. teacher"""
        preservation = {}
        for feat in features:
            me_teacher = compute_marginal_effect(teacher, X, feat)
            me_student = compute_marginal_effect(student, X, feat)

            # Pearson correlation
            corr = np.corrcoef(me_teacher, me_student)[0, 1]
            preservation[feat] = corr

        return np.mean(list(preservation.values()))

    @staticmethod
    def economic_interpretability_score(model, constraints, stability_results):
        """Aggregate score of economic interpretability"""
        # Compliance with constraints
        w1 = 0.4
        compliance = constraint_compliance_rate(...)

        # Coefficient stability
        w2 = 0.3
        avg_cv = np.mean(stability_results['cv'])
        stability_score = max(0, 1 - avg_cv / 0.15)

        # Sign stability
        w3 = 0.3
        sign_score = np.mean(stability_results['sign_stability'])

        score = w1 * compliance + w2 * stability_score + w3 * sign_score
        return score * 100  # 0-100%
\end{lstlisting}

\subsection{Performance Optimizations}

\subsubsection{Caching Teacher Probabilities}

Pre-computing teacher probabilities avoids re-predictions:

\begin{lstlisting}[language=Python]
# Cache teacher probabilities
teacher_probs = teacher.predict_proba(X_train)
np.save('teacher_probs.npy', teacher_probs)

# Reuse in bootstrap
for b in range(n_bootstrap):
    X_boot, p_boot = bootstrap_sample(X_train, teacher_probs)
    student.fit(X_boot, p_boot)
\end{lstlisting}

\subsubsection{Bootstrap Parallelization}

\begin{lstlisting}[language=Python]
from joblib import Parallel, delayed

def fit_bootstrap_sample(distiller, X, y, p, indices):
    return distiller.fit(X[indices], y[indices], p[indices])

# Parallelize
coefficients = Parallel(n_jobs=-1)(
    delayed(fit_bootstrap_sample)(distiller, X, y, p,
                                   bootstrap_indices(n))
    for _ in range(n_bootstrap)
)
\end{lstlisting}

\subsection{Integration with DeepBridge Workflow}

Framework integrates with existing DeepBridge pipeline:

\begin{lstlisting}[language=Python, caption=Complete Pipeline]
from deepbridge.distillation import AutoDistiller
from deepbridge.distillation.economics import *

# 1. Load dataset
dataset = DBDataset.from_csv('credit_data.csv')

# 2. Train teacher via AutoDistiller
auto_distiller = AutoDistiller(
    dataset=dataset,
    method='hpm'  # Advanced distillation
)
teacher = auto_distiller.best_model()

# 3. Configure economic distillation
constraints = EconomicConstraints()
constraints.add_sign('income', -1)
constraints.add_sign('interest_rate', +1)
constraints.add_monotonicity('age', 'increasing')

econ_distiller = EconomicDistiller(
    teacher=teacher,
    constraints=constraints,
    student_type=ModelType.GAM_CLASSIFIER
)

# 4. Fit with stability analysis
student = econ_distiller.fit(X_train, y_train)
stability = StabilityAnalyzer().analyze(econ_distiller, X_train, y_train)

# 5. Generate economic report
report = EconomicReport(student, stability, constraints)
report.save('economic_analysis.pdf')
\end{lstlisting}
